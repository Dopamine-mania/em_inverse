"""
æ”¹è¿›ç‰ˆDeepONetè®­ç»ƒè„šæœ¬ - ä¿®å¤ç‰ˆæœ¬
1. çœŸæ­£çš„æ‰¹å¤„ç†è®­ç»ƒï¼ˆæ”¯æŒå˜é•¿åºåˆ—ï¼‰
2. å¹³è¡¡çš„æŸå¤±æƒé‡
3. æ”¹è¿›çš„å­¦ä¹ ç‡è°ƒåº¦
4. ä¼˜åŒ–çš„æ•°æ®å¤„ç†
5. è‡ªå®šä¹‰collateå‡½æ•°å¤„ç†å˜é•¿åºåˆ—
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import argparse
import os
import multiprocessing as mp
from pathlib import Path
import sys
import time
import json
from typing import Tuple, Dict, List, Optional
from torch.utils.data import DataLoader, Dataset
from torch.utils.tensorboard import SummaryWriter  # æ·»åŠ TensorBoardæ”¯æŒ

# è®¾ç½®CUDAç¯å¢ƒå˜é‡ä»¥ä¼˜åŒ–å¤šGPUæ€§èƒ½
os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,3'  # æ’é™¤è¢«å ç”¨çš„GPU 2
os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'

# ä¿®å¤PyTorch 2.0+çš„CUDA generatoré—®é¢˜
# è®¾ç½®é»˜è®¤éšæœºæ•°ç”Ÿæˆå™¨ä¸ºCPU
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(42)
    torch.cuda.empty_cache()  # æ¸…ç†GPUç¼“å­˜

# è®¾ç½®multiprocessing start methodä¸ºspawnä»¥æ”¯æŒCUDA
if __name__ == '__main__':
    try:
        mp.set_start_method('spawn', force=True)
        print("Multiprocessing start method set to 'spawn' for CUDA compatibility")
    except RuntimeError:
        print("Multiprocessing start method already set")

# å¼ºåˆ¶è®¾ç½®PyTorchçš„å…¨å±€generatorä¸ºCPUè®¾å¤‡
import torch.utils.data as data
data.get_worker_info = lambda: None  # ç¦ç”¨worker info

# æ·»åŠ æ¨¡å—è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from config import Config
from data.dataset import MaskedDeepONetDataset
from model.model import create_pytorch_dual_branch_deeponet  # åŸå§‹ç‰ˆæœ¬ï¼Œä¿æŒå…¼å®¹æ€§
from model.enhanced_deeponet import create_enhanced_deeponet  # å¢å¼ºç‰ˆæœ¬ï¼ˆæ¨èä½¿ç”¨ï¼‰
from loss.loss import PyTorchLossFunction
from utils.utils import (
    save_checkpoint, load_checkpoint, visualize_predictions,
    calculate_metrics, print_metrics, set_random_seed, get_model_summary
)


class DynamicDeepONetDataset(Dataset):
    """åŠ¨æ€åŠ è½½çš„DeepONetæ•°æ®é›†ç±» - é¿å…ä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰æ•°æ®"""

    def __init__(self, samples_list: List[Dict], cfg: Config):
        self.samples_list = samples_list
        self.cfg = cfg
        self.dataset = MaskedDeepONetDataset(cfg)

    def __len__(self):
        return len(self.samples_list)

    def __getitem__(self, idx):
        sample = self.samples_list[idx]
        try:
            # åŠ¨æ€å¤„ç†å•ä¸ªæ ·æœ¬ - æ·»åŠ å›ºå®šæ¢é’ˆé€‰æ‹©é€‰é¡¹ç”¨äºè°ƒè¯•
            freq_idx = sample.get('freq_idx', 0)

            # è°ƒè¯•æ¨¡å¼ï¼šä½¿ç”¨å›ºå®šçš„sample_idxç¡®ä¿æ¢é’ˆé€‰æ‹©ä¸€è‡´
            # æ­£å¸¸è®­ç»ƒï¼šä½¿ç”¨éšæœºsample_idxå¢åŠ æ•°æ®å¤šæ ·æ€§
            import random
            sample_idx = idx if getattr(self, '_fixed_probe_mode', False) else random.randint(0, 1000000)

            if getattr(self, '_fixed_probe_mode', False):
                print(f"[æ’æŸ¥] å›ºå®šæ¢é’ˆæ¨¡å¼: sample_idx={sample_idx} (idx={idx})")

            # ä¿®æ”¹ï¼šdataset.prepare_single_sampleç°åœ¨è¿”å›å•branchæ ¼å¼
            branch_input, trunk_input, target_output, mask, probe_coords_3d = self.dataset.prepare_single_sample(
                sample['file'], freq_idx=freq_idx, sample_idx=sample_idx
            )

            # è½¬æ¢ä¸ºå¼ é‡å¹¶é‡å¡‘ (å•Branchæ ¼å¼: 25*4+1=101ç»´)
            expected_branch_size = self.cfg.deeponet.probe_count * 4 + 1  # ä¿®å¤ï¼šå•Branchè¾“å…¥ (x,y,real,imag) * 25 + freq
            branch_tensor = torch.from_numpy(branch_input).float().view(-1)  # [101]
            trunk_tensor = torch.from_numpy(trunk_input).float()  # [N, 3]
            target_tensor = torch.from_numpy(target_output).float()  # [N, 2]
            mask_tensor = torch.from_numpy(mask).bool()  # [N]
            probe_coords_tensor = torch.from_numpy(probe_coords_3d).float()  # [25, 3]

            return {
                'branch_input': branch_tensor,  # [101] å•Branchè¾“å…¥
                'trunk': trunk_tensor,  # [N, 3] = (x, y, frequency)
                'y': target_tensor,     # [N, 2] = (real, imag)
                'mask': mask_tensor,    # [N] = probe positions in trunk
                'probe_coords': probe_coords_tensor,  # [25, 3] æ¢é’ˆåæ ‡ï¼ˆç”¨äºSpectral Lossï¼‰
                'sample_idx': idx  # æ ·æœ¬ç´¢å¼•ï¼ˆç”¨äºSpectral Lossç¼“å­˜ï¼‰
            }
        except Exception as e:
            print(f"åŠ¨æ€åŠ è½½æ ·æœ¬å¤±è´¥ {sample.get('source_file', sample['file'])}: {e}")
            # è¿”å›ä¸€ä¸ªç©ºçš„æ ·æœ¬è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            expected_branch_size = self.cfg.deeponet.probe_count * 4 + 1  # å•Branchæ ¼å¼
            print(f"[æ’æŸ¥] è¿”å›ç©ºæ ·æœ¬: branch_size={expected_branch_size}")
            return {
                'branch_input': torch.zeros(expected_branch_size),
                'trunk': torch.zeros(1, 3),
                'y': torch.zeros(1, 2),
                'mask': torch.zeros(1, dtype=torch.bool),
                'probe_coords': torch.zeros(self.cfg.data.num_probes, 3),
                'sample_idx': idx
            }


def collate_fn(batch, cfg=None):
    """
    è‡ªå®šä¹‰collateå‡½æ•°å¤„ç†å˜é•¿åºåˆ— - å•Branchç‰ˆæœ¬ï¼Œæ”¯æŒSpectral Loss
    Args:
        batch: List of Dict with keys: 'branch_input', 'trunk', 'y', 'mask', 'probe_coords', 'sample_idx'
        cfg: Config é…ç½®å¯¹è±¡ï¼Œç”¨äºæ§åˆ¶è­¦å‘Šè¡Œä¸º
    Returns:
        Dict with padded sequences, probe coords, and sample IDs for Spectral Loss
    """
    batch_size = len(batch)

    # åˆ†ç¦»å„ä¸ªå­—æ®µ
    branch_data = [item['branch_input'] for item in batch]  # List of [101]
    trunk_data = [item['trunk'] for item in batch]          # List of [N_i, 3]
    y_data = [item['y'] for item in batch]                 # List of [N_i, 2]
    masks = [item['mask'] for item in batch]               # List of [N_i]
    probe_coords_data = [item['probe_coords'] for item in batch]  # List of [25, 3]
    sample_ids = [item['sample_idx'] for item in batch]   # List of int

    # Branchæ•°æ®å †å ï¼ˆéƒ½æ˜¯å›ºå®šé•¿åº¦ï¼‰
    branch_tensor = torch.stack(branch_data, dim=0)  # [batch_size, 101]

    # æ‰¾åˆ°trunkå’Œyçš„æœ€å¤§é•¿åº¦
    max_trunk_length = max(item.shape[0] for item in trunk_data)
    max_mask_length = max(item.shape[0] for item in masks)

    # åˆå§‹åŒ–å¡«å……åçš„å¼ é‡
    trunk_padded = torch.zeros(batch_size, max_trunk_length, 3)  # [batch, max_trunk_len, 3]
    y_padded = torch.zeros(batch_size, max_trunk_length, 2)       # [batch, max_trunk_len, 2]
    mask_padded = torch.zeros(batch_size, max_mask_length, dtype=torch.bool)  # [batch, max_mask_len]

    # æå–çœŸå®æ¢é’ˆå€¼ - ç”¨äºRBFæ ¡æ­£
    probe_true_values_list = []
    expected_probe_count = None  # æœŸæœ›çš„æ¢é’ˆæ•°é‡
    enable_warnings = cfg is not None and cfg.physics.probe_alignment_warning

    # å¡«å……æ•°æ®
    trunk_lengths = []
    for i, (trunk, y, mask) in enumerate(zip(trunk_data, y_data, masks)):
        trunk_length = trunk.shape[0]
        mask_length = mask.shape[0]
        trunk_lengths.append(trunk_length)

        # å¡«å……trunkå’Œyæ•°æ®
        trunk_padded[i, :trunk_length] = trunk
        y_padded[i, :trunk_length] = y

        # å¡«å……maskæ•°æ® - ä½¿ç”¨maskçš„å®é™…é•¿åº¦
        mask_padded[i, :mask_length] = mask

        # æå–æ¢é’ˆä½ç½®çš„çœŸå®å€¼
        probe_mask = mask.bool()  # [total_points_i]
        probe_values_i = y[probe_mask]  # [num_probes, 2]

        # æ£€æŸ¥æ¢é’ˆå¯¹é½æƒ…å†µ
        actual_probe_count = probe_values_i.shape[0]

        if expected_probe_count is None:
            # ç¬¬ä¸€ä¸ªæ ·æœ¬è®¾ç½®æœŸæœ›æ•°é‡
            expected_probe_count = actual_probe_count

        if actual_probe_count == 0:
            if enable_warnings:
                print(f"è­¦å‘Š: æ ·æœ¬ {i} ä¸­æ²¡æœ‰æ¢é’ˆç‚¹ (maskä¸ºç©º)ï¼Œå°†ä½¿ç”¨é›¶å¡«å……")
            # ä½¿ç”¨é…ç½®çš„æ¢é’ˆæ•°é‡ä½œä¸ºæœŸæœ›æ•°é‡
            if expected_probe_count == 0:
                expected_probe_count = 50  # ä»é…ç½®æ¨æ–­ï¼Œæˆ–ç¡¬ç¼–ç é»˜è®¤å€¼
                if cfg is not None:
                    expected_probe_count = cfg.deeponet.probe_count
        elif actual_probe_count != expected_probe_count:
            if enable_warnings:
                print(f"è­¦å‘Š: æ ·æœ¬ {i} æ¢é’ˆæ•°é‡ä¸åŒ¹é… - æœŸæœ›: {expected_probe_count}, å®é™…: {actual_probe_count}")

            if actual_probe_count > expected_probe_count:
                if enable_warnings:
                    print(f"   æˆªæ–­å‰ {actual_probe_count - expected_probe_count} ä¸ªæ¢é’ˆ")
                probe_values_i = probe_values_i[:expected_probe_count]
            else:
                if enable_warnings:
                    print(f"   å¡«å…… {expected_probe_count - actual_probe_count} ä¸ªé›¶å€¼æ¢é’ˆ")
                padding = torch.zeros(expected_probe_count - actual_probe_count, 2)
                probe_values_i = torch.cat([probe_values_i, padding], dim=0)

        probe_true_values_list.append(probe_values_i)

    # æœ€ç»ˆæ£€æŸ¥æ¢é’ˆæ•°é‡ä¸€è‡´æ€§
    final_probe_counts = [values.shape[0] for values in probe_true_values_list]
    if not all(count == final_probe_counts[0] for count in final_probe_counts):
        if enable_warnings:
            print(f"é”™è¯¯: æ¢é’ˆæ•°é‡ä¸ä¸€è‡´ - å„æ ·æœ¬æ¢é’ˆæ•°: {final_probe_counts}")
            print(f"   è¿™å¯èƒ½å¯¼è‡´RBFæ ¡æ­£é”™ä½ï¼Œè¯·æ£€æŸ¥æ•°æ®ç”Ÿæˆè¿‡ç¨‹")

        # å¼ºåˆ¶å¯¹é½åˆ°æœ€å°æ•°é‡ï¼Œé¿å…æ›´ä¸¥é‡çš„é”™ä½
        min_probe_count = min(final_probe_counts)
        if enable_warnings:
            print(f"   å¼ºåˆ¶å¯¹é½åˆ°æœ€å°æ¢é’ˆæ•°é‡: {min_probe_count}")
        probe_true_values_list = [values[:min_probe_count] for values in probe_true_values_list]

    # å †å æ¢é’ˆçœŸå®å€¼
    probe_true_values = torch.stack(probe_true_values_list, dim=0)  # [batch_size, probe_count, 2]

    # å †å æ¢é’ˆåæ ‡å’Œæ ·æœ¬IDï¼ˆç”¨äºSpectral Lossï¼‰
    probe_coords_tensor = torch.stack(probe_coords_data, dim=0)  # [batch_size, 25, 3]
    sample_ids_tensor = torch.tensor(sample_ids, dtype=torch.long)  # [batch_size]

    # æœ€ç»ˆéªŒè¯ï¼ˆä»…åœ¨å¯ç”¨è­¦å‘Šæ—¶æ˜¾ç¤ºï¼‰
    if enable_warnings:
        batch_size, actual_probe_count, _ = probe_true_values.shape
        print(f"æ¢é’ˆæ•°æ®éªŒè¯:")
        print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   æ¢é’ˆæ•°é‡: {actual_probe_count}")
        if probe_true_values.numel() > 0:
            print(f"   æ¢é’ˆçœŸå€¼èŒƒå›´: [{probe_true_values.min():.6f}, {probe_true_values.max():.6f}]")
        else:
            print(f"   æ¢é’ˆçœŸå€¼èŒƒå›´: ç©ºå¼ é‡")

    return {
        'branch_input': branch_tensor,  # [batch_size, 101] å•Branchè¾“å…¥
        'trunk': trunk_padded,
        'y': y_padded,
        'mask': mask_padded,
        'lengths': torch.tensor(trunk_lengths),
        'probe_true_values': probe_true_values,  # çœŸå®æ¢é’ˆå€¼ç”¨äºRBFæ ¡æ­£
        'probe_coords': probe_coords_tensor,  # [batch_size, 25, 3] ç”¨äºSpectral Loss
        'sample_ids': sample_ids_tensor  # [batch_size] ç”¨äºSpectral Lossç¼“å­˜
    }


class DeepONetBatchDataset(Dataset):
    """çœŸæ­£çš„æ‰¹å¤„ç†æ•°æ®é›†"""

    def __init__(self, branch_data, trunk_data, y_data, masks):
        self.branch_data = branch_data  # List of [1, 50, 5]
        self.trunk_data = trunk_data    # List of [N_i, 3]
        self.y_data = y_data           # List of [N_i, 2]
        self.masks = masks             # List of [N_i]

    def __len__(self):
        return len(self.branch_data)

    def __getitem__(self, idx):
        branch_data = self.branch_data[idx]  # [1, 50, 5] æˆ– [50, 5] æˆ–å…¶ä»–å½¢çŠ¶

        # ç¡®ä¿branch_dataæ˜¯æ­£ç¡®çš„ç»´åº¦
        if branch_data.dim() == 3:
            # [1, 50, 5] -> [50, 5]
            branch_data = branch_data.squeeze(0)

        # é‡å¡‘ä¸º[250]
        branch_data = branch_data.view(-1)  # [50, 5] -> [250]

        return {
            'branch': branch_data,              # [250]
            'trunk': self.trunk_data[idx],      # [N_i, 3]
            'y': self.y_data[idx],             # [N_i, 2]
            'mask': self.masks[idx]            # [N_i]
        }


# åœ¨ç±»å¼€å§‹å¤„æ·»åŠ é…ç½®å‚æ•°
class ImprovedDeepONetTrainer:
    """æ”¹è¿›ç‰ˆDeepONetè®­ç»ƒå™¨ - ä¿®å¤ç‰ˆæœ¬"""

    def __init__(self, cfg: Config):
        self.cfg = cfg

        # æ€§èƒ½ä¼˜åŒ–é…ç½®
        self.max_sample_length = 1000  # å¯é…ç½®çš„æœ€å¤§æ ·æœ¬é•¿åº¦
        self.use_adaptive_sampling = True  # æ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”é‡‡æ ·
        self.cfg = cfg
        # æ£€æŸ¥å¯ç”¨çš„GPUè®¾å¤‡
        print(f"=== æ£€æŸ¥GPUå¯ç”¨æ€§ ===")
        print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")

        available_gpus = []
        for i in range(torch.cuda.device_count()):  # æ£€æŸ¥æ‰€æœ‰å¯ç”¨GPU
            try:
                # æµ‹è¯•GPUæ˜¯å¦å¯ç”¨
                torch.cuda.set_device(i)
                test_tensor = torch.cuda.FloatTensor(1)
                available_gpus.append(i)
                print(f"GPU {i}: {torch.cuda.get_device_name(i)} - å¯ç”¨")
            except Exception as e:
                print(f"GPU {i}: ä¸å¯ç”¨ - {e}")

        if not available_gpus:
            print("æœªæ‰¾åˆ°å¯ç”¨çš„GPUï¼Œä½¿ç”¨CPU")
            self.device = torch.device('cpu')
            self.multi_gpu = False
        else:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„GPUä½œä¸ºä¸»è®¾å¤‡
            main_gpu = available_gpus[0]
            self.device = torch.device(f'cuda:{main_gpu}')
            print(f"ä½¿ç”¨ä¸»GPU: {main_gpu}")
            print(f"å¯ç”¨GPUåˆ—è¡¨: {available_gpus}")

            # å¦‚æœæœ‰å¤šä¸ªå¯ç”¨GPUï¼Œå¯ç”¨DataParallel
            if len(available_gpus) > 1:
                print(f"å¯ç”¨å¤šGPUè®­ç»ƒ (ä½¿ç”¨ {len(available_gpus)} ä¸ªGPU)")
                self.multi_gpu = True

                # ç”±äºè®¾ç½®äº†CUDA_VISIBLE_DEVICES='0,1,3'ï¼ŒPyTorchçœ‹åˆ°çš„GPUç´¢å¼•æ˜¯0,1,2
                # ä½†æˆ‘ä»¬éœ€è¦è·³è¿‡åŸæ¥è¢«å ç”¨çš„GPU 2ï¼Œæ‰€ä»¥ä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU
                self.gpu_ids = list(range(len(available_gpus)))  # ä½¿ç”¨0,1,2å¯¹åº”å®é™…çš„0,1,3
                print(f"ä½¿ç”¨GPUç´¢å¼•æ˜ å°„: {self.gpu_ids} (å¯¹åº”å®é™…GPU: [0,1,3])")
            else:
                print("ä½¿ç”¨å•GPUè®­ç»ƒ")
                self.multi_gpu = False

        # è®¾ç½®éšæœºç§å­
        set_random_seed(42)

        # åˆå§‹åŒ–æ¨¡å‹
        # ä½¿ç”¨å¢å¼ºç‰ˆDeepONetï¼Œæ”¯æŒæ®‹å·®è¿æ¥ã€æ³¨æ„åŠ›ã€Dropoutç­‰é«˜çº§åŠŸèƒ½
        print(f"ä½¿ç”¨å¢å¼ºç‰ˆDeepONetç½‘ç»œ")
        print(f"ç½‘ç»œé¢„è®¾: {cfg.deeponet.network_preset}")
        print(f"æ¿€æ´»å‡½æ•°: {cfg.deeponet.activation}")
        print(f"Dropoutç‡: {cfg.deeponet.dropout_rate}")
        print(f"æ®‹å·®è¿æ¥: {'å¯ç”¨' if cfg.deeponet.use_residual else 'ç¦ç”¨'}")
        print(f"æ³¨æ„åŠ›æœºåˆ¶: {'å¯ç”¨' if cfg.deeponet.use_attention else 'ç¦ç”¨'}")

        self.model = create_enhanced_deeponet(cfg)

        # å¢å¼ºçš„æ¨¡å‹æ¶æ„æ‘˜è¦
        print("\n=== æ¨¡å‹æ¶æ„æ‘˜è¦ ===")
        print(f"ç½‘ç»œé¢„è®¾: {cfg.deeponet.network_preset}")
        print(f"éšè—å±‚ç»“æ„: {cfg.deeponet.hidden_layers}")
        print(f"è¾“å‡ºç»´åº¦: {cfg.deeponet.output_dim}")
        print(f"æ¿€æ´»å‡½æ•°: {cfg.deeponet.activation}")
        print(f"Dropoutç‡: {cfg.deeponet.dropout_rate}")
        print(f"æ³¨æ„åŠ›æœºåˆ¶: {'å¯ç”¨' if cfg.deeponet.use_attention else 'ç¦ç”¨'}")
        print(f"æ®‹å·®è¿æ¥: {'å¯ç”¨' if cfg.deeponet.use_residual else 'ç¦ç”¨'}")
        print(f"RBFæ ¡æ­£: {'å¯ç”¨' if cfg.physics.enable_rbf_correction else 'ç¦ç”¨'}")
        print(f"æ¢é’ˆæ•°é‡: {cfg.data.num_probes}")

        # æ˜¾ç¤ºç½‘ç»œé…ç½®ä¿¡æ¯
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"æ€»å‚æ•°æ•°é‡: {total_params:,}")
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"ç½‘ç»œç»“æ„: {cfg.deeponet.hidden_layers} -> {cfg.deeponet.output_dim}")
        print("=" * 40)

        self.model.to(self.device)

        # å¯ç”¨å¤šGPUè®­ç»ƒ
        if hasattr(self, 'multi_gpu') and self.multi_gpu:
            print(f"æ­£åœ¨å°†æ¨¡å‹åˆ†å¸ƒåˆ°GPU: {self.gpu_ids}")
            self.model = torch.nn.DataParallel(self.model, device_ids=self.gpu_ids)
            print(f"DataParallelåŒ…è£…å®Œæˆ")
            print(f"æ¨¡å‹ç±»å‹: {type(self.model)}")
            print(f"æ¨¡å‹å‚æ•°æ€»æ•°: {sum(p.numel() for p in self.model.parameters()):,}")
            print(f"å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in self.model.parameters() if p.requires_grad):,}")
        else:
            print("ä½¿ç”¨å•GPUæ¨¡å¼")
            print(f"æ¨¡å‹å‚æ•°æ€»æ•°: {sum(p.numel() for p in self.model.parameters()):,}")
            print(f"å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in self.model.parameters() if p.requires_grad):,}")

        # ä¼˜åŒ–çš„ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ï¼ˆç§»é™¤verboseå‚æ•°ï¼‰
        if hasattr(self, 'multi_gpu') and self.multi_gpu:
            # DataParallelä¼šåŒ…è£…æ¨¡å‹ï¼Œéœ€è¦è®¿é—®.moduleå±æ€§
            self.optimizer = optim.Adam(
                self.model.module.parameters(),
                lr=cfg.training.learning_rate,
                weight_decay=1e-5  # æ¢å¤æ­£å¸¸æƒé‡è¡°å‡
            )
        else:
            self.optimizer = optim.Adam(
                self.model.parameters(),
                lr=cfg.training.learning_rate,
                weight_decay=1e-5  # æ¢å¤æ­£å¸¸æƒé‡è¡°å‡
            )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨ - ä¿®å¤ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.8, patience=20
        )

        # æŸå¤±å‡½æ•°
        self.loss_fn = PyTorchLossFunction(cfg)

        # åˆå§‹åŒ–TensorBoard writer
        log_dir = Path(self.cfg.visualization.output_dir) / "tensorboard"
        log_dir.mkdir(parents=True, exist_ok=True)
        self.writer = SummaryWriter(log_dir=log_dir)
        print(f"TensorBoardæ—¥å¿—ç›®å½•: {log_dir}")
        print(f"å¯åŠ¨å‘½ä»¤: tensorboard --logdir {log_dir}")

        print(f"æ”¹è¿›ç‰ˆè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"æŸå¤±æƒé‡ - Probe: {cfg.physics.probe_loss_weight}, "
              f"Field: {cfg.physics.field_loss_weight}, "
              f"Smooth: {cfg.physics.smoothness_loss_weight}")
        get_model_summary(self.model)

    def train_epoch_optimized(self, dataloader, epoch: int) -> Tuple[float, Dict[str, float]]:
        """ä¼˜åŒ–ç‰ˆè®­ç»ƒepoch - çœŸæ­£çš„æ‰¹é‡å¤„ç†"""
        self.model.train()

        # ç§»é™¤æ¯ä¸ªepochçš„torch.manual_seed(42)è°ƒç”¨ï¼Œé¿å…éšæœºæ€§ä¸€è‡´å¯¼è‡´dropoutå¤±æ•ˆ
        # å…¨å±€éšæœºç§å­å·²åœ¨è®­ç»ƒå™¨åˆå§‹åŒ–æ—¶é€šè¿‡set_random_seed(42)è®¾ç½®ä¸€æ¬¡

        epoch_losses = {'total': 0, 'probe': 0, 'field': 0, 'correlation': 0, 'smooth': 0, 'spectral_loss': 0}
        num_batches = 0

        print(f"[DEBUG] Starting epoch {epoch}, dataloader length: {len(dataloader)}")

        try:
            # è®¡æ—¶å˜é‡
            data_load_time = 0
            model_forward_time = 0
            loss_compute_time = 0
            backward_time = 0
            optimizer_time = 0

            for batch_idx, batch in enumerate(dataloader):
                # æ•°æ®ç§»åŠ¨è®¡æ—¶å¼€å§‹
                start_time = time.time()

                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡ - å•Branchæ ¼å¼
                branch_input_batch = batch['branch_input'].to(self.device)  # [batch_size, 101]
                trunk_batch = batch['trunk'].to(self.device)             # [batch_size, max_len, 3]
                y_batch = batch['y'].to(self.device)                    # [batch_size, max_len, 2]
                mask_batch = batch['mask'].to(self.device)               # [batch_size, max_len]
                lengths = batch['lengths']                               # [batch_size]
                probe_coords = batch['probe_coords'].to(self.device)     # [batch_size, 25, 3]
                sample_ids = batch['sample_ids'].to(self.device)         # [batch_size]

                data_load_time += time.time() - start_time

                current_batch_size = branch_input_batch.shape[0]

                # æ¸…é›¶æ¢¯åº¦
                self.optimizer.zero_grad()

                # ä¼˜åŒ–ç‰ˆæ‰¹é‡å¤„ç†ï¼šç§»é™¤æ ·æœ¬çº§åˆ«çš„forå¾ªç¯
                batch_loss = 0.0
                loss_dict_batch = {'probe_loss': 0, 'field_loss': 0, 'smooth_loss': 0}

                # æ¨¡å‹å‰å‘ä¼ æ’­è®¡æ—¶å¼€å§‹
                forward_start = time.time()

                # ç›´æ¥æ‰¹é‡é¢„æµ‹ - å•Branchè¾“å…¥ï¼Œä¼ é€’çœŸå®æ¢é’ˆå€¼ç”¨äºRBFæ ¡æ­£
                probe_true_values = batch.get('probe_true_values', None)  # [batch_size, probe_count, 2]

                if probe_true_values is not None:
                    y_pred = self.model(branch_input_batch, trunk_batch,
                                      mask=mask_batch, lengths=lengths,
                                      probe_true_values=probe_true_values)
                else:
                    # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰æ¢é’ˆå€¼ï¼Œä½¿ç”¨åŸæ¥çš„è°ƒç”¨æ–¹å¼
                    y_pred = self.model(branch_input_batch, trunk_batch,
                                      mask=mask_batch, lengths=lengths)

                model_forward_time += time.time() - forward_start

                # æŸå¤±è®¡ç®—è®¡æ—¶å¼€å§‹
                loss_start = time.time()

                # æ‰¹é‡æŸå¤±è®¡ç®— - ä¼ é€’coordså’Œsample_idsç”¨äºSpectral Loss
                loss, loss_dict = self.loss_fn(
                    y_pred, y_batch,
                    [mask_batch[i] for i in range(mask_batch.shape[0])],
                    lengths,
                    coords=trunk_batch,      # [batch_size, max_len, 3] ç©ºé—´åæ ‡
                    sample_ids=sample_ids    # [batch_size] æ ·æœ¬IDç”¨äºSpectral Lossç¼“å­˜
                )

                # æ¢é’ˆmaskç»Ÿè®¡ - åªåœ¨ç¬¬ä¸€ä¸ªæ‰¹æ¬¡æ‰“å°
                if batch_idx == 0:
                    for i in range(min(2, mask_batch.shape[0])):  # åªæ£€æŸ¥å‰2ä¸ªæ ·æœ¬
                        mask_sum = mask_batch[i].sum().item()
                        expected_probes = self.cfg.data.num_probes
                        match_status = "OK" if mask_sum == expected_probes else "FAIL"

                loss_compute_time += time.time() - loss_start

                # åå‘ä¼ æ’­è®¡æ—¶å¼€å§‹
                backward_start = time.time()

                # åå‘ä¼ æ’­
                loss.backward()

                # æ¢¯åº¦è£å‰ª - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œå¤„ç†DataParallelæƒ…å†µ
                if hasattr(self, 'multi_gpu') and self.multi_gpu:
                    torch.nn.utils.clip_grad_norm_(self.model.module.parameters(), max_norm=1.0)
                else:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                backward_time += time.time() - backward_start

                # ä¼˜åŒ–å™¨è®¡æ—¶å¼€å§‹
                optimizer_start = time.time()

                # æ›´æ–°å‚æ•°
                self.optimizer.step()

                optimizer_time += time.time() - optimizer_start

                # è®°å½•æŸå¤±
                batch_loss = loss.item()
                epoch_losses['total'] += batch_loss

                # å®‰å…¨åœ°æå–æŸå¤±å€¼ï¼ˆå¤„ç†tensorå’Œfloatç±»å‹ï¼‰
                if hasattr(loss_dict['probe_loss'], 'item'):
                    epoch_losses['probe'] += loss_dict['probe_loss'].item()
                else:
                    epoch_losses['probe'] += float(loss_dict['probe_loss'])

                if hasattr(loss_dict['field_loss'], 'item'):
                    epoch_losses['field'] += loss_dict['field_loss'].item()
                else:
                    epoch_losses['field'] += float(loss_dict['field_loss'])

                if hasattr(loss_dict['correlation_loss'], 'item'):
                    epoch_losses['correlation'] += loss_dict['correlation_loss'].item()
                else:
                    epoch_losses['correlation'] += float(loss_dict['correlation_loss'])

                if hasattr(loss_dict['smooth_loss'], 'item'):
                    epoch_losses['smooth'] += loss_dict['smooth_loss'].item()
                else:
                    epoch_losses['smooth'] += float(loss_dict['smooth_loss'])

                # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ·»åŠ spectral lossèšåˆ
                if hasattr(loss_dict['spectral_loss'], 'item'):
                    epoch_losses['spectral_loss'] += loss_dict['spectral_loss'].item()
                else:
                    epoch_losses['spectral_loss'] += float(loss_dict['spectral_loss'])

                num_batches += 1

                # æ‰“å°è¿›åº¦å’Œè®¡æ—¶ä¿¡æ¯
                if batch_idx % max(1, len(dataloader) // 10) == 0:
                    # è·å–åŸå§‹æŸå¤±å€¼
                    probe_val = loss_dict['probe_loss'].item() if hasattr(loss_dict['probe_loss'], 'item') else float(loss_dict['probe_loss'])
                    field_val = loss_dict['field_loss'].item() if hasattr(loss_dict['field_loss'], 'item') else float(loss_dict['field_loss'])
                    correlation_val = loss_dict['correlation_loss'].item() if hasattr(loss_dict['correlation_loss'], 'item') else float(loss_dict['correlation_loss'])
                    smooth_val = loss_dict['smooth_loss'].item() if hasattr(loss_dict['smooth_loss'], 'item') else float(loss_dict['smooth_loss'])

                    # è®¡ç®—åŠ æƒè´¡çŒ®å€¼ï¼ˆå®é™…è´¡çŒ®åˆ°æ€»lossçš„å€¼ï¼‰
                    probe_contribution = self.cfg.physics.probe_loss_weight * probe_val
                    field_contribution = self.cfg.physics.field_loss_weight * field_val
                    correlation_contribution = self.cfg.physics.correlation_loss_weight * correlation_val
                    smooth_contribution = self.cfg.physics.smoothness_loss_weight * smooth_val

                    # è¾“å‡ºåŠ æƒåçš„å®é™…è´¡çŒ®å€¼ - å¢å¼ºæ¢é’ˆç›‘æ§
                    print(f"Epoch {epoch + 1}/{self.cfg.training.epochs} "
                          f"Batch{batch_idx}/{len(dataloader)} "
                          f"æ¢é’ˆåŸå§‹MSE: {probe_val:.6f} â†’ åŠ æƒ: {probe_contribution:.4f}, "
                          f"åœºåŸå§‹MSE: {field_val:.6f} â†’ åŠ æƒ: {field_contribution:.4f}, "
                          f"ç›¸å…³æ€§: {correlation_contribution:.4f}, "
                          f"æ€»loss: {batch_loss:.4f}")

            # å¹³å‡epochæŸå¤±
            if num_batches > 0:
                for key in epoch_losses:
                    epoch_losses[key] /= num_batches
            else:
                print(f"[WARNING] No batches processed in epoch {epoch}")
                return 0.0, epoch_losses

            return epoch_losses['total'], epoch_losses

        except Exception as e:
            print(f"[DEBUG] Exception during optimized training: {e}")
            print(f"[DEBUG] Exception type: {type(e)}")
            raise e

    def save_training_visualizations(self, epoch, train_loader, test_loader, train_loss, test_loss):
        """æ¯2ä¸ªepochä¿å­˜è®­ç»ƒå¯è§†åŒ–å›¾ç‰‡ - æ”¹è¿›ç‰ˆ (2x4å¸ƒå±€)"""
        self.model.eval()

        with torch.no_grad():
            # éšæœºé€‰æ‹©ä¸åŒçš„æ‰¹æ¬¡æ¥æ˜¾ç¤ºä¸åŒæ„å‹
            import random
            train_random_idx = random.randint(0, len(train_loader) - 1)
            test_random_idx = random.randint(0, len(test_loader) - 1)

            # è·å–éšæœºæ‰¹æ¬¡
            train_batches = list(train_loader)
            test_batches = list(test_loader)
            train_batch = train_batches[train_random_idx]
            test_batch = test_batches[test_random_idx]

            # å¤„ç†è®­ç»ƒæ•°æ® - éšæœºé€‰æ‹©ä¸€ä¸ªæ ·æœ¬ï¼ˆå•Branchæ ¼å¼ï¼‰
            sample_idx = random.randint(0, train_batch['branch_input'].shape[0] - 1)
            train_branch_input = train_batch['branch_input'][sample_idx:sample_idx+1].to(self.device)
            train_trunk = train_batch['trunk'][sample_idx:sample_idx+1].to(self.device)
            train_y = train_batch['y'][sample_idx:sample_idx+1].to(self.device)
            train_mask = train_batch['mask'][sample_idx:sample_idx+1].to(self.device)
            train_lengths = train_batch['lengths'][sample_idx:sample_idx+1]

            # å¤„ç†æµ‹è¯•æ•°æ® - éšæœºé€‰æ‹©ä¸€ä¸ªæ ·æœ¬ï¼ˆå•Branchæ ¼å¼ï¼‰
            test_sample_idx = random.randint(0, test_batch['branch_input'].shape[0] - 1)
            test_branch_input = test_batch['branch_input'][test_sample_idx:test_sample_idx+1].to(self.device)
            test_trunk = test_batch['trunk'][test_sample_idx:test_sample_idx+1].to(self.device)
            test_y = test_batch['y'][test_sample_idx:test_sample_idx+1].to(self.device)
            test_mask = test_batch['mask'][test_sample_idx:test_sample_idx+1].to(self.device)
            test_lengths = test_batch['lengths'][test_sample_idx:test_sample_idx+1]

            # åˆ›å»ºå¯è§†åŒ–å›¾ç‰‡ - 2x4å¸ƒå±€
            import matplotlib.pyplot as plt
            import matplotlib
            matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
            from matplotlib.patches import Circle
            import matplotlib.patches as patches

            # å­—ä½“è®¾ç½®å·²é€šè¿‡utilsæ¨¡å—å…¨å±€è®¾ç½®ï¼Œè¿™é‡Œç¡®ä¿ä¸€ä¸‹
            if 'SimHei' not in plt.rcParams['font.sans-serif']:
                plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
            if plt.rcParams['axes.unicode_minus']:
                plt.rcParams['axes.unicode_minus'] = False

            fig, axes = plt.subplots(2, 4, figsize=(24, 12))
            fig.suptitle(f'Epoch {epoch} - Training Progress (æ„å‹: Trainæ ·æœ¬{sample_idx+1}, Testæ ·æœ¬{test_sample_idx+1})\n'
                        f'Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}', fontsize=14)

            # è®­ç»ƒé›†å¯è§†åŒ–
            train_length = train_lengths[0].item()
            train_coords = train_trunk[0, :train_length, :2].cpu().numpy()
            train_true_real = train_y[0, :train_length, 0].cpu().numpy()  # å®éƒ¨
            train_true_imag = train_y[0, :train_length, 1].cpu().numpy()  # è™šéƒ¨

            # ä½¿ç”¨æ­£ç¡®çš„é•¿åº¦è¿›è¡Œé¢„æµ‹ï¼ˆå•Branchæ ¼å¼ï¼‰
            train_trunk_single = train_trunk[0, :train_length].unsqueeze(0)
            train_pred_single = self._predict_single_sample(train_branch_input, train_trunk_single)
            train_pred_real = train_pred_single[:, 0]  # é¢„æµ‹å®éƒ¨
            train_pred_imag = train_pred_single[:, 1]  # é¢„æµ‹è™šéƒ¨

            train_error_real = np.abs(train_true_real - train_pred_real)
            train_error_imag = np.abs(train_true_imag - train_pred_imag)

            # ä»branchæ•°æ®ä¸­æå–æ¢é’ˆä½ç½®ï¼ˆå•Branchæ ¼å¼ï¼š[x1,y1,r1,i1,...,freq]ï¼‰
            train_branch_data = train_branch_input[0].cpu().numpy()
            # è®¡ç®—å®é™…çš„æ¢é’ˆæ•°é‡ï¼š(branchæ•°æ®é•¿åº¦-1)/4ï¼Œæœ€å1ä½æ˜¯é¢‘ç‡
            actual_num_probes = (len(train_branch_data) - 1) // 4
            # æå–x,yåæ ‡ï¼šæ¯4ä¸ªå…ƒç´ å–å‰2ä¸ª
            train_probe_coords = []
            for i in range(actual_num_probes):
                train_probe_coords.append([train_branch_data[i*4], train_branch_data[i*4+1]])
            train_probe_coords = np.array(train_probe_coords)  # [num_probes, 2]

            # è®­ç»ƒé›† - çœŸå®å€¼å®éƒ¨ (å¸¦colorbar)
            scatter1 = axes[0, 0].scatter(train_coords[:, 0], train_coords[:, 1],
                                        c=train_true_real, cmap='viridis', s=5, alpha=0.6)
            axes[0, 0].set_title('Training Set - True Real Values')
            axes[0, 0].set_xlabel('X')
            axes[0, 0].set_ylabel('Y')
            plt.colorbar(scatter1, ax=axes[0, 0], label='çœŸå®åœºå¼º(å®éƒ¨)')

            # æ·»åŠ æ¢é’ˆä½ç½®æ ‡æ³¨
            self._add_probe_markers(axes[0, 0], train_probe_coords)  # é»˜è®¤æ˜¾ç¤ºæ‰€æœ‰æ¢é’ˆ

            # æ·»åŠ æ•°æ®ç‚¹çš„å‡¸è¾¹ç•Œ
            self._add_convex_boundary(axes[0, 0], train_coords, alpha=0.3, color='blue', label='æ•°æ®è¾¹ç•Œ')

            # æ·»åŠ æ¢é’ˆçš„å‡¸è¾¹ç•Œ
            self._add_convex_boundary(axes[0, 0], train_probe_coords, alpha=0.5, color='green', label='æ¢é’ˆè¾¹ç•Œ')

            # æ·»åŠ å›¾ä¾‹
            axes[0, 0].legend(loc='upper right', fontsize=8)

            # è®­ç»ƒé›† - é¢„æµ‹å€¼å®éƒ¨ (å¸¦colorbar)
            scatter2 = axes[0, 1].scatter(train_coords[:, 0], train_coords[:, 1],
                                        c=train_pred_real, cmap='viridis', s=5, alpha=0.6)
            axes[0, 1].set_title('Training Set - Predicted Real Values')
            axes[0, 1].set_xlabel('X')
            axes[0, 1].set_ylabel('Y')
            plt.colorbar(scatter2, ax=axes[0, 1], label='é¢„æµ‹åœºå¼º(å®éƒ¨)')

            # æ·»åŠ æ¢é’ˆä½ç½®å’Œè¾¹ç•Œ
            self._add_probe_markers(axes[0, 1], train_probe_coords)  # é»˜è®¤æ˜¾ç¤ºæ‰€æœ‰æ¢é’ˆ
            self._add_convex_boundary(axes[0, 1], train_coords, alpha=0.3, color='blue', label='æ•°æ®è¾¹ç•Œ')
            self._add_convex_boundary(axes[0, 1], train_probe_coords, alpha=0.5, color='green', label='æ¢é’ˆè¾¹ç•Œ')

            # è®­ç»ƒé›† - è¯¯å·®å®éƒ¨ (å¸¦colorbar)
            scatter3 = axes[0, 2].scatter(train_coords[:, 0], train_coords[:, 1],
                                        c=train_error_real, cmap='Reds', s=5, alpha=0.6)
            axes[0, 2].set_title(f'Training Set - Real Error (MAE: {np.mean(train_error_real):.6f})')
            axes[0, 2].set_xlabel('X')
            axes[0, 2].set_ylabel('Y')
            plt.colorbar(scatter3, ax=axes[0, 2], label='å®éƒ¨è¯¯å·®')

            # è®­ç»ƒé›† - å®éƒ¨ç›¸å…³æ€§æ•£ç‚¹å›¾
            correlation_real = np.corrcoef(train_true_real, train_pred_real)[0, 1]
            axes[0, 3].scatter(train_true_real, train_pred_real, alpha=0.6, s=2)
            axes[0, 3].plot([train_true_real.min(), train_true_real.max()],
                           [train_true_real.min(), train_true_real.max()], 'r--', lw=2)
            axes[0, 3].set_xlabel('True Real Values')
            axes[0, 3].set_ylabel('Predicted Real Values')
            axes[0, 3].set_title(f'Training Set - Real Correlation: {correlation_real:.6f}')
            axes[0, 3].grid(True, alpha=0.3)

            # æµ‹è¯•é›†å¯è§†åŒ–
            test_length = test_lengths[0].item()
            test_coords = test_trunk[0, :test_length, :2].cpu().numpy()
            test_true_real = test_y[0, :test_length, 0].cpu().numpy()  # å®éƒ¨
            test_true_imag = test_y[0, :test_length, 1].cpu().numpy()  # è™šéƒ¨

            # ä½¿ç”¨æ­£ç¡®çš„é•¿åº¦è¿›è¡Œé¢„æµ‹ï¼ˆå•Branchæ ¼å¼ï¼‰
            test_trunk_single = test_trunk[0, :test_length].unsqueeze(0)
            test_pred_single = self._predict_single_sample(test_branch_input, test_trunk_single)
            test_pred_real = test_pred_single[:, 0]  # é¢„æµ‹å®éƒ¨
            test_pred_imag = test_pred_single[:, 1]  # é¢„æµ‹è™šéƒ¨

            test_error_real = np.abs(test_true_real - test_pred_real)
            test_error_imag = np.abs(test_true_imag - test_pred_imag)

            # ä»branchæ•°æ®ä¸­æå–æ¢é’ˆä½ç½®ï¼ˆå•Branchæ ¼å¼ï¼š[x1,y1,r1,i1,...,freq]ï¼‰
            test_branch_data = test_branch_input[0].cpu().numpy()
            # è®¡ç®—å®é™…çš„æ¢é’ˆæ•°é‡ï¼š(branchæ•°æ®é•¿åº¦-1)/4ï¼Œæœ€å1ä½æ˜¯é¢‘ç‡
            actual_num_probes_test = (len(test_branch_data) - 1) // 4
            # æå–x,yåæ ‡ï¼šæ¯4ä¸ªå…ƒç´ å–å‰2ä¸ª
            test_probe_coords = []
            for i in range(actual_num_probes_test):
                test_probe_coords.append([test_branch_data[i*4], test_branch_data[i*4+1]])
            test_probe_coords = np.array(test_probe_coords)  # [num_probes, 2]

            # æµ‹è¯•é›† - çœŸå®å€¼å®éƒ¨ (å¸¦colorbar)
            scatter4 = axes[1, 0].scatter(test_coords[:, 0], test_coords[:, 1],
                                        c=test_true_real, cmap='viridis', s=5, alpha=0.6)
            axes[1, 0].set_title('Test Set - True Real Values')
            axes[1, 0].set_xlabel('X')
            axes[1, 0].set_ylabel('Y')
            plt.colorbar(scatter4, ax=axes[1, 0], label='çœŸå®åœºå¼º(å®éƒ¨)')

            self._add_probe_markers(axes[1, 0], test_probe_coords)  # é»˜è®¤æ˜¾ç¤ºæ‰€æœ‰æ¢é’ˆ
            self._add_convex_boundary(axes[1, 0], test_coords, alpha=0.3, color='blue', label='æ•°æ®è¾¹ç•Œ')
            self._add_convex_boundary(axes[1, 0], test_probe_coords, alpha=0.5, color='green', label='æ¢é’ˆè¾¹ç•Œ')

            # æµ‹è¯•é›† - é¢„æµ‹å€¼å®éƒ¨ (å¸¦colorbar)
            scatter5 = axes[1, 1].scatter(test_coords[:, 0], test_coords[:, 1],
                                        c=test_pred_real, cmap='viridis', s=5, alpha=0.6)
            axes[1, 1].set_title('Test Set - Predicted Real Values')
            axes[1, 1].set_xlabel('X')
            axes[1, 1].set_ylabel('Y')
            plt.colorbar(scatter5, ax=axes[1, 1], label='é¢„æµ‹åœºå¼º(å®éƒ¨)')

            self._add_probe_markers(axes[1, 1], test_probe_coords)  # é»˜è®¤æ˜¾ç¤ºæ‰€æœ‰æ¢é’ˆ
            self._add_convex_boundary(axes[1, 1], test_coords, alpha=0.3, color='blue', label='æ•°æ®è¾¹ç•Œ')
            self._add_convex_boundary(axes[1, 1], test_probe_coords, alpha=0.5, color='green', label='æ¢é’ˆè¾¹ç•Œ')

            # æµ‹è¯•é›† - è¯¯å·®å®éƒ¨ (å¸¦colorbar)
            scatter6 = axes[1, 2].scatter(test_coords[:, 0], test_coords[:, 1],
                                        c=test_error_real, cmap='Reds', s=5, alpha=0.6)
            axes[1, 2].set_title(f'Test Set - Real Error (MAE: {np.mean(test_error_real):.6f})')
            axes[1, 2].set_xlabel('X')
            axes[1, 2].set_ylabel('Y')
            plt.colorbar(scatter6, ax=axes[1, 2], label='å®éƒ¨è¯¯å·®')

            # æµ‹è¯•é›† - å®éƒ¨ç›¸å…³æ€§æ•£ç‚¹å›¾
            correlation_test_real = np.corrcoef(test_true_real, test_pred_real)[0, 1]
            axes[1, 3].scatter(test_true_real, test_pred_real, alpha=0.6, s=2)
            axes[1, 3].plot([test_true_real.min(), test_true_real.max()],
                           [test_true_real.min(), test_true_real.max()], 'r--', lw=2)
            axes[1, 3].set_xlabel('True Real Values')
            axes[1, 3].set_ylabel('Predicted Real Values')
            axes[1, 3].set_title(f'Test Set - Real Correlation: {correlation_test_real:.6f}')
            axes[1, 3].grid(True, alpha=0.3)

            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯åˆ°ç›¸å…³æ€§å›¾
            correlation_text = (
                f"è®­ç»ƒé›†ç»Ÿè®¡:\n"
                f"å®éƒ¨ç›¸å…³æ€§: {correlation_real:.6f}\n"
                f"å®éƒ¨MAE: {np.mean(train_error_real):.6f}\n"
                f"å®éƒ¨æœ€å¤§è¯¯å·®: {np.max(train_error_real):.6f}\n\n"
                f"æµ‹è¯•é›†ç»Ÿè®¡:\n"
                f"å®éƒ¨ç›¸å…³æ€§: {correlation_test_real:.6f}\n"
                f"å®éƒ¨MAE: {np.mean(test_error_real):.6f}\n"
                f"å®éƒ¨æœ€å¤§è¯¯å·®: {np.max(test_error_real):.6f}"
            )

            # åœ¨å³ä¸‹è§’å›¾ä¸­æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            axes[1, 3].text(0.02, 0.98, correlation_text, transform=axes[1, 3].transAxes,
                           fontsize=8, verticalalignment='top',
                           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))

            plt.tight_layout()

            # ä¿å­˜å›¾ç‰‡
            save_path = Path(self.cfg.visualization.output_dir) / "training_progress"
            save_path.mkdir(parents=True, exist_ok=True)
            plt.savefig(save_path / f'epoch_{epoch:04d}.png', dpi=150, bbox_inches='tight')
            plt.close()

            print(f"å·²ä¿å­˜è®­ç»ƒè¿›åº¦å›¾ç‰‡: epoch_{epoch:04d}.png (2x4å¸ƒå±€)")

    def plot_loss_curves(self, loss_history):
        """ç»˜åˆ¶lossæ›²çº¿"""
        try:
            import matplotlib.pyplot as plt
            import matplotlib
            matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯

            # è®¾ç½®ä¸­æ–‡å­—ä½“
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
            try:
                matplotlib.font_manager.fontManager.addfont(
                    matplotlib.font_manager.FontProperties(family='SimHei')
                )
            except:
                pass  # å¦‚æœå­—ä½“ä¸å­˜åœ¨ï¼Œè·³è¿‡

            epochs = range(1, len(loss_history['train_total']) + 1)

            # åˆ›å»º2x2å­å›¾
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))

            # 1. æ€»losså¯¹æ¯”
            axes[0, 0].plot(epochs, loss_history['train_total'], 'b-', label='è®­ç»ƒæ€»loss', linewidth=2)
            axes[0, 0].plot(epochs, loss_history['test_total'], 'r-', label='æµ‹è¯•æ€»loss', linewidth=2)
            axes[0, 0].set_title('æ€»losså˜åŒ–')
            axes[0, 0].set_xlabel('Epoch')
            axes[0, 0].set_ylabel('Loss')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)

            # 2. å„é¡¹è®­ç»ƒloss
            axes[0, 1].plot(epochs, loss_history['train_probe'], 'g-', label='æ¢é’ˆloss', linewidth=2)
            axes[0, 1].plot(epochs, loss_history['train_field'], 'orange', label='åœºloss', linewidth=2)
            axes[0, 1].plot(epochs, loss_history['train_correlation'], 'purple', label='ç›¸å…³æ€§loss', linewidth=2)
            if loss_history['train_smooth'][-1] > 0:
                axes[0, 1].plot(epochs, loss_history['train_smooth'], 'brown', label='å¹³æ»‘loss', linewidth=2)
            axes[0, 1].set_title('å„é¡¹è®­ç»ƒlosså˜åŒ–')
            axes[0, 1].set_xlabel('Epoch')
            axes[0, 1].set_ylabel('Loss')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)

            # 3. è®­ç»ƒvsæµ‹è¯•æ€»loss (å¯¹æ•°åæ ‡)
            axes[1, 0].semilogy(epochs, loss_history['train_total'], 'b-', label='è®­ç»ƒæ€»loss', linewidth=2)
            axes[1, 0].semilogy(epochs, loss_history['test_total'], 'r-', label='æµ‹è¯•æ€»loss', linewidth=2)
            axes[1, 0].set_title('æ€»losså˜åŒ– (å¯¹æ•°åæ ‡)')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('Loss (log scale)')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)

            # 4. æŸå¤±å æ¯”åˆ†æ
            train_probe_contrib = [self.cfg.physics.probe_loss_weight * p for p in loss_history['train_probe']]
            train_field_contrib = [self.cfg.physics.field_loss_weight * f for f in loss_history['train_field']]
            train_correlation_contrib = [self.cfg.physics.correlation_loss_weight * c for c in loss_history['train_correlation']]

            axes[1, 1].stackplot(epochs, train_probe_contrib, train_field_contrib, train_correlation_contrib,
                                 labels=['æ¢é’ˆè´¡çŒ®', 'åœºè´¡çŒ®', 'ç›¸å…³æ€§è´¡çŒ®'], alpha=0.7)
            axes[1, 1].set_title('æŸå¤±ç»„æˆå æ¯”')
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('Loss Contribution')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)

            plt.tight_layout()

            # ä¿å­˜å›¾ç‰‡
            save_path = Path(self.cfg.visualization.output_dir) / "loss_curves"
            save_path.mkdir(parents=True, exist_ok=True)
            plt.savefig(save_path / 'loss_curves.png', dpi=300, bbox_inches='tight')
            plt.close()

            print(f"å·²ä¿å­˜lossæ›²çº¿å›¾: {save_path}/loss_curves.png")

            # è¾“å‡ºæœ€ä½³ç»“æœ
            best_train_idx = np.argmin(loss_history['train_total'])
            best_test_idx = np.argmin(loss_history['test_total'])

            print(f"\n=== è®­ç»ƒç»“æœæ€»ç»“ ===")
            print(f"æœ€ä½³è®­ç»ƒloss: {loss_history['train_total'][best_train_idx]:.6f} (Epoch {best_train_idx + 1})")
            print(f"æœ€ä½³æµ‹è¯•loss: {loss_history['test_total'][best_test_idx]:.6f} (Epoch {best_test_idx + 1})")
            print(f"æœ€ç»ˆè®­ç»ƒloss: {loss_history['train_total'][-1]:.6f}")
            print(f"æœ€ç»ˆæµ‹è¯•loss: {loss_history['test_total'][-1]:.6f}")

        except Exception as e:
            print(f"ç»˜åˆ¶lossæ›²çº¿å¤±è´¥: {e}")

    def _predict_single_sample(self, branch_input, trunk):
        """é¢„æµ‹å•ä¸ªæ ·æœ¬ - å•Branchç‰ˆæœ¬è¿”å›å®éƒ¨å’Œè™šéƒ¨"""
        # æœŸæœ›çš„branchç»´åº¦ï¼š25 probes Ã— 4 features + 1 freq = 101
        expected_branch_size = self.cfg.deeponet.probe_count * 4 + 1

        # é‡æ„branch_inputç»´åº¦
        if branch_input.dim() == 2:
            branch_flat = branch_input[0].view(-1)
        else:
            branch_flat = branch_input.view(-1)

        # ç¡®ä¿branch_flatæ˜¯æ­£ç¡®çš„å¤§å° [101]
        if branch_flat.numel() != expected_branch_size:
            if branch_flat.numel() > expected_branch_size:
                branch_flat = branch_flat[:expected_branch_size]
            else:
                padding = torch.zeros(expected_branch_size - branch_flat.numel(), device=branch_flat.device)
                branch_flat = torch.cat([branch_flat, padding])

        # é‡æ„trunkç»´åº¦
        if trunk.dim() == 3:
            trunk = trunk.squeeze(0)
            seq_length = trunk.shape[0]
        elif trunk.dim() == 2:
            seq_length = trunk.shape[0]
        else:
            seq_length = 1000

        # å‡†å¤‡æ¨¡å‹è¾“å…¥ - ä¿æŒæ­£ç¡®çš„å½¢çŠ¶
        branch_input_shaped = branch_flat.unsqueeze(0)  # [1, 101]
        trunk_input = trunk.unsqueeze(0) if trunk.dim() == 2 else trunk.unsqueeze(0)  # [1, seq_length, 3]

        # æ¨¡å‹é¢„æµ‹ï¼ˆå•Branchè¾“å…¥ï¼‰
        try:
            with torch.no_grad():
                predictions = self.model(branch_input_shaped, trunk_input)
                return predictions[0].cpu().numpy()  # è¿”å› [seq_length, 2]

        except Exception as e:
            print(f"[DEBUG] Prediction error: {e}")
            # è¿”å›é›¶æ•°ç»„ä½œä¸ºfallback
            return np.zeros((seq_length, 2))

    def _add_probe_markers(self, ax, probe_coords, step=1):
        """æ·»åŠ æ¢é’ˆä½ç½®æ ‡æ³¨"""
        for i, (px, py) in enumerate(probe_coords[::step]):  # æ¯stepä¸ªæ˜¾ç¤ºä¸€ä¸ªæ¢é’ˆ
            ax.scatter(px, py, c='red', s=20, marker='^', alpha=0.8,
                      edgecolors='white', linewidth=1)
            if i < 10:  # åªæ˜¾ç¤ºå‰10ä¸ªçš„æ ‡ç­¾
                ax.annotate(f'P{i*step}', (px, py), xytext=(2, 2),
                          textcoords='offset points', fontsize=6, color='red')

    def _add_convex_boundary(self, ax, coords, alpha=0.3, color='blue', label=None):
        """æ·»åŠ å‡¸è¾¹ç•Œæ˜¾ç¤º"""
        try:
            from scipy.spatial import ConvexHull
            import numpy as np

            # è®¡ç®—å‡¸åŒ…
            hull = ConvexHull(coords)

            # ç»˜åˆ¶å‡¸åŒ…è¾¹ç•Œçº¿
            boundary_line = None
            for simplex in hull.simplices:
                line = ax.plot(coords[simplex, 0], coords[simplex, 1],
                             color=color, alpha=alpha, linewidth=2)
                if boundary_line is None:
                    boundary_line = line[0]

            # å¡«å……å‡¸åŒ…åŒºåŸŸ
            ax.fill(coords[hull.vertices, 0], coords[hull.vertices, 1],
                   color=color, alpha=alpha*0.3, label=label if label else None)

        except Exception as e:
            # å¦‚æœscipyä¸å¯ç”¨æˆ–è®¡ç®—å¤±è´¥ï¼Œç»˜åˆ¶ç®€å•çš„è¾¹ç•Œæ¡†
            print(f"å‡¸åŒ…è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨è¾¹ç•Œæ¡†: {e}")
            x_min, x_max = np.min(coords[:, 0]), np.max(coords[:, 0])
            y_min, y_max = np.min(coords[:, 1]), np.max(coords[:, 1])

            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            from matplotlib.patches import Rectangle
            rect = Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,
                           linewidth=2, edgecolor=color, facecolor=color,
                           alpha=alpha*0.3, label=label if label else None)
            ax.add_patch(rect)

    def _predict_single_sample(self, branch, trunk):
        """é¢„æµ‹å•ä¸ªæ ·æœ¬"""
        # æœŸæœ›çš„branchç»´åº¦
        expected_branch_size = self.cfg.deeponet.probe_count * 5

        # é‡æ„branchç»´åº¦
        if branch.dim() == 3:
            # [1, num_probes, 5] -> [probe_count * 5]
            branch_flat = branch.squeeze(0).view(-1)
        elif branch.dim() == 2:
            # [batch_size, probe_count * 5] -> åªå–ç¬¬ä¸€ä¸ªæ ·æœ¬ [probe_count * 5]
            branch_flat = branch[0].view(-1)
        else:
            # [probe_count * 5] ç›´æ¥ä½¿ç”¨
            branch_flat = branch.view(-1)

        # ç¡®ä¿branch_flatæ˜¯æ­£ç¡®çš„å¤§å° [probe_count * 5]
        if branch_flat.numel() != expected_branch_size:
            if branch_flat.numel() == 50 * 5:  # æ—§çš„50æ¢é’ˆæ•°æ®
                # å¦‚æœæ˜¯250ä½†æœŸæœ›æ›´å¤§ï¼Œéœ€è¦é‡å¤å¡«å……
                if expected_branch_size > 250:
                    repeats = expected_branch_size // 250
                    remainder = expected_branch_size % 250
                    branch_flat = branch_flat.repeat(repeats)
                    if remainder > 0:
                        branch_flat = torch.cat([branch_flat, branch_flat[:remainder]])
                else:
                    branch_flat = branch_flat.view(-1)
            else:
                print(f"[DEBUG] Unexpected branch shape in _predict_single_sample: {branch_flat.shape}")
                # å¦‚æœå…ƒç´ ä¸å¯¹ï¼Œæˆªæ–­æˆ–å¡«å……åˆ°expected_branch_size
                if branch_flat.numel() > expected_branch_size:
                    branch_flat = branch_flat[:expected_branch_size]
                else:
                    padding = torch.zeros(expected_branch_size - branch_flat.numel(), device=branch_flat.device)
                    branch_flat = torch.cat([branch_flat, padding])

        # é‡æ„trunkç»´åº¦
        if trunk.dim() == 3:
            # [1, N, 3] -> [N, 3]
            trunk = trunk.squeeze(0)
            seq_length = trunk.shape[0]
        elif trunk.dim() == 2:
            seq_length = trunk.shape[0]
        else:
            print(f"[DEBUG] Unexpected trunk shape: {trunk.shape}")
            seq_length = 1000  # é»˜è®¤é•¿åº¦

        # å‡†å¤‡æ¨¡å‹è¾“å…¥ - ä¿æŒæ­£ç¡®çš„å½¢çŠ¶ï¼Œä¸è¦é”™è¯¯æ‰©å±•
        branch_input = branch_flat.unsqueeze(0)  # [1, probe_count * 5]
        trunk_input = trunk.unsqueeze(0)  # [1, seq_length, 3]

        # æ¨¡å‹é¢„æµ‹
        try:
            with torch.no_grad():
                predictions = self.model(branch_input, trunk_input)
                return predictions[0].cpu().numpy()  # è¿”å› [seq_length, 2]

        except Exception as e:
            print(f"[DEBUG] Prediction error in _predict_single_sample: {e}")
            # è¿”å›é›¶æ•°ç»„ä½œä¸ºfallback
            return np.zeros((seq_length, 2))

    def evaluate_optimized(self, dataloader) -> float:
        """ä¼˜åŒ–ç‰ˆè¯„ä¼°å‡½æ•° - çœŸæ­£çš„æ‰¹é‡å¤„ç†"""
        self.model.eval()

        test_losses = {'total': 0, 'probe': 0, 'field': 0, 'smooth': 0}
        num_batches = 0

        with torch.no_grad():
            for batch in dataloader:
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡ - å•Branchæ ¼å¼
                branch_input_batch = batch['branch_input'].to(self.device)
                trunk_batch = batch['trunk'].to(self.device)
                y_batch = batch['y'].to(self.device)
                mask_batch = batch['mask'].to(self.device)
                lengths = batch['lengths'].to(self.device)
                probe_coords = batch['probe_coords'].to(self.device)
                sample_ids = batch['sample_ids'].to(self.device)

                # è·å–çœŸå®æ¢é’ˆå€¼
                probe_true_values = batch.get('probe_true_values', None)
                if probe_true_values is not None:
                    probe_true_values = probe_true_values.to(self.device)

                current_batch_size = branch_input_batch.shape[0]

                # æ‰¹é‡é¢„æµ‹ - å•Branchè¾“å…¥ï¼Œä¼ é€’çœŸå®æ¢é’ˆå€¼ç”¨äºRBFæ ¡æ­£
                if probe_true_values is not None:
                    y_pred = self.model(branch_input_batch, trunk_batch,
                                      mask=mask_batch, lengths=lengths,
                                      probe_true_values=probe_true_values)
                else:
                    y_pred = self.model(branch_input_batch, trunk_batch,
                                      mask=mask_batch, lengths=lengths)

                # æ‰¹é‡æŸå¤±è®¡ç®— - ä¼ é€’coordså’Œsample_idsç”¨äºSpectral Loss
                loss, loss_dict = self.loss_fn(
                    y_pred, y_batch,
                    [mask_batch[i] for i in range(mask_batch.shape[0])],
                    lengths,
                    coords=trunk_batch,
                    sample_ids=sample_ids
                )

                # è®°å½•
                test_losses['total'] += loss.item()

                # å®‰å…¨åœ°æå–æŸå¤±å€¼ï¼ˆå¤„ç†tensorå’Œfloatç±»å‹ï¼‰
                if hasattr(loss_dict['probe_loss'], 'item'):
                    test_losses['probe'] += loss_dict['probe_loss'].item()
                else:
                    test_losses['probe'] += float(loss_dict['probe_loss'])

                if hasattr(loss_dict['field_loss'], 'item'):
                    test_losses['field'] += loss_dict['field_loss'].item()
                else:
                    test_losses['field'] += float(loss_dict['field_loss'])

                if hasattr(loss_dict['smooth_loss'], 'item'):
                    test_losses['smooth'] += loss_dict['smooth_loss'].item()
                else:
                    test_losses['smooth'] += float(loss_dict['smooth_loss'])

                num_batches += 1

        # å¹³å‡æŸå¤±
        if num_batches > 0:
            for key in test_losses:
                test_losses[key] /= num_batches
        else:
            print(f"[WARNING] No batches processed in evaluation")
            return 0.0

        return test_losses['total']

    def train_optimized(self, train_loader, test_loader):
        """ä¼˜åŒ–ç‰ˆè®­ç»ƒä¸»å¾ªç¯ - çœŸæ­£çš„æ‰¹é‡å¤„ç†"""
        print(f"\n=== å¼€å§‹ä¼˜åŒ–ç‰ˆè®­ç»ƒ ===")
        print(f"è®­ç»ƒè½®æ•°: {self.cfg.training.epochs}")
        print(f"å…³é”®ä¼˜åŒ–ï¼šç§»é™¤é€ç‚¹å¤„ç†å’Œé€æ ·æœ¬å¤„ç†forå¾ªç¯")

        # æ‰“å°å…³é”®è®­ç»ƒé…ç½®
        print(f"\n=== è®­ç»ƒé…ç½®ç¡®è®¤ ===")
        print(f"æ•°æ®è·¯å¾„: {self.cfg.data.data_path}")
        print(f"æ¢é’ˆæ•°é‡: {self.cfg.data.num_probes}")
        print(f"æ‰¹æ¬¡å¤§å°: {self.cfg.training.batch_size}")
        print(f"å­¦ä¹ ç‡: {self.cfg.training.learning_rate}")
        print(f"ç½‘ç»œé¢„è®¾: {self.cfg.deeponet.network_preset}")
        print(f"Dropout: {self.cfg.deeponet.dropout_rate}")
        print(f"æ³¨æ„åŠ›: {'å¯ç”¨' if self.cfg.deeponet.use_attention else 'ç¦ç”¨'}")
        print(f"æ®‹å·®: {'å¯ç”¨' if self.cfg.deeponet.use_residual else 'ç¦ç”¨'}")
        print(f"RBFæ ¡æ­£: {'å¯ç”¨' if self.cfg.physics.enable_rbf_correction else 'ç¦ç”¨'}")
        print("=" * 60)

        best_test_loss = float('inf')
        patience_counter = 0
        train_losses = []
        test_losses = []

        # è¯¦ç»†çš„lossè·Ÿè¸ªï¼ˆåŒ…å«Spectral Lossï¼‰
        loss_history = {
            'train_total': [],
            'test_total': [],
            'train_probe': [],
            'train_field': [],
            'train_correlation': [],
            'train_smooth': [],
            'train_spectral': []  # Day 1æ ¸å¿ƒï¼šSpectral Lossè·Ÿè¸ª
        }

        for epoch in range(self.cfg.training.epochs):
            epoch_start_time = time.time()

            # è®­ç»ƒ
            train_loss, train_loss_details = self.train_epoch_optimized(train_loader, epoch + 1)

            # è¯„ä¼°
            test_loss = self.evaluate_optimized(test_loader)

            # è®°å½•æŸå¤±å†å²ï¼ˆåŒ…å«Spectral Lossï¼‰
            loss_history['train_total'].append(train_loss)
            loss_history['test_total'].append(test_loss)
            loss_history['train_probe'].append(train_loss_details['probe'])
            loss_history['train_field'].append(train_loss_details['field'])
            loss_history['train_correlation'].append(train_loss_details['correlation'])
            loss_history['train_smooth'].append(train_loss_details['smooth'])
            loss_history['train_spectral'].append(train_loss_details.get('spectral_loss', 0.0))  # Day 1æ ¸å¿ƒ

            # TensorBoardæ—¥å¿—è®°å½•
            self.writer.add_scalar("Loss/Train_Total", train_loss, epoch)
            self.writer.add_scalar("Loss/Test_Total", test_loss, epoch)
            self.writer.add_scalar("Loss/Train_Probe", train_loss_details['probe'], epoch)
            self.writer.add_scalar("Loss/Train_Field", train_loss_details['field'], epoch)
            self.writer.add_scalar("Loss/Train_Correlation", train_loss_details['correlation'], epoch)
            self.writer.add_scalar("Loss/Train_Smooth", train_loss_details['smooth'], epoch)
            self.writer.add_scalar("Loss/Train_Spectral", train_loss_details.get('spectral_loss', 0.0), epoch)  # Day 1æ ¸å¿ƒ

            # å­¦ä¹ ç‡æ—¥å¿—
            current_lr = self.optimizer.param_groups[0]['lr']
            self.writer.add_scalar("Learning_Rate", current_lr, epoch)

            # å‚æ•°æ¢¯åº¦èŒƒæ•°ï¼ˆå¯é€‰ï¼‰
            total_norm = 0
            for name, param in self.model.named_parameters():
                if param.grad is not None:
                    param_norm = param.grad.data.norm(2)
                    total_norm += param_norm.item() ** 2
            total_norm = total_norm ** (1. / 2)
            self.writer.add_scalar("Gradients/Total_Norm", total_norm, epoch)

            self.writer.flush()  # ç«‹å³å†™å…¥

            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step(test_loss)
            current_lr = self.optimizer.param_groups[0]['lr']

            # è®°å½•æŸå¤±
            train_losses.append(train_loss)
            test_losses.append(test_loss)

            epoch_time = time.time() - epoch_start_time

            # è®¡ç®—Pure MSE (Data Only, ä¸å«Spectral Loss)
            pure_mse_train = (train_loss_details['probe'] +
                             train_loss_details['field'] +
                             train_loss_details['correlation'] +
                             train_loss_details['smooth'])
            spectral_train = train_loss_details.get('spectral_loss', 0.0)

            # æ¯ä¸ªepochéƒ½æ˜¾ç¤ºè¯¦ç»†Lossæ‹†åˆ†
            print(f"[Epoch {epoch + 1}/{self.cfg.training.epochs}] "
                  f"Total: {train_loss:.4f} | "
                  f"Pure MSE (Data): {pure_mse_train:.4f} | "
                  f"Spectral: {spectral_train:.4f} | "
                  f"Test: {test_loss:.4f}")

            # æ‰“å°è¯¦ç»†è¿›åº¦ï¼ˆæ¯display_everyä¸ªepochï¼‰
            if (epoch + 1) % self.cfg.training.display_every == 0:
                print(f"Epoch {epoch + 1}/{self.cfg.training.epochs} è¯¦ç»†æ‹†åˆ†:")
                print(f"  è®­ç»ƒ Total Loss: {train_loss:.6f}")
                print(f"  è®­ç»ƒ Pure MSE (Data Only): {pure_mse_train:.6f}")
                print(f"    â”œâ”€ Probe: {train_loss_details['probe']:.6f}")
                print(f"    â”œâ”€ Field: {train_loss_details['field']:.6f}")
                print(f"    â”œâ”€ Correlation: {train_loss_details['correlation']:.6f}")
                print(f"    â””â”€ Smooth: {train_loss_details['smooth']:.6f}")
                print(f"  è®­ç»ƒ Spectral Loss (k-space): {spectral_train:.6f}")
                print(f"  æµ‹è¯• Total Loss: {test_loss:.6f}")
                print(f"  å½’ä¸€åŒ–çŠ¶æ€: normalize_data=False (åŸå§‹ç‰©ç†æ•°å€¼)")
                print("-" * 60)

            # æ¯plot_frequencyä¸ªepochä¿å­˜å¯è§†åŒ–å›¾ç‰‡
            if (epoch + 1) % self.cfg.training.plot_frequency == 0:
                try:
                    self.save_training_visualizations(epoch + 1, train_loader, test_loader, train_loss, test_loss)
                except Exception as e:
                    # å¦‚æœå¯è§†åŒ–å¤±è´¥ï¼Œç®€å•è·³è¿‡ï¼Œä¸å½±å“è®­ç»ƒ
                    pass
            else:
                # è·³è¿‡å¯è§†åŒ–ä¿å­˜ï¼Œä¸å½±å“è®­ç»ƒ
                pass

            # æ—©åœå’Œä¿å­˜
            if test_loss < best_test_loss:
                best_test_loss = test_loss
                patience_counter = 0

                # åªåœ¨æµ‹è¯•æŸå¤±åˆ›æ–°ä½æ—¶ä¿å­˜æœ€ä¼˜æ¨¡å‹
                print(f"[NEW BEST] æœ€ä½³æµ‹è¯•æŸå¤±: {best_test_loss:.6f} (Epoch {epoch + 1})")
                save_checkpoint(
                    self.model, self.optimizer,
                    epoch=epoch + 1,
                    loss_history=train_losses,
                    cfg=self.cfg,
                    save_dir=self.cfg.data.checkpoint_dir,
                    model_type="pytorch_optimized",
                    is_best_model=True  # æ ‡è®°ä¸ºæœ€ä½³æ¨¡å‹
                )
            else:
                patience_counter += 1
                if patience_counter >= self.cfg.training.early_stopping_patience:
                    print(f"æ—©åœè§¦å‘ï¼Œåœ¨Epoch {epoch + 1}")
                    break

        total_time = time.time() - epoch_start_time
        print(f"ä¼˜åŒ–ç‰ˆè®­ç»ƒå®Œæˆï¼æ€»ç”¨æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
        print(f"æœ€ä½³æµ‹è¯•æŸå¤±: {best_test_loss:.6f}")

        # ç»˜åˆ¶lossæ›²çº¿
        self.plot_loss_curves(loss_history)

        # å…³é—­TensorBoard writer
        if hasattr(self, 'writer'):
            self.writer.close()
            print(f"TensorBoardæ—¥å¿—å·²å…³é—­")

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_epoch = len(train_losses)
        print(f"\n[TRAINING COMPLETE] ä¿å­˜æœ€ç»ˆæ¨¡å‹ (Epoch {final_epoch})")
        save_checkpoint(
            self.model, self.optimizer,
            epoch=final_epoch,
            loss_history=train_losses,
            cfg=self.cfg,
            save_dir=self.cfg.data.checkpoint_dir,
            model_type="pytorch_optimized",
            is_final=True  # æ ‡è®°ä¸ºæœ€ç»ˆæ¨¡å‹
        )

        return train_losses, test_losses


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='ä¿®å¤ç‰ˆDeepONetç”µç£åœºé‡æ„è®­ç»ƒ')

    # é…ç½®æ–‡ä»¶å‚æ•°
    parser.add_argument('--config_file', type=str, default='config.yaml',
                       help='YAMLé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--preset', type=str, default=None,
                       help='ä½¿ç”¨çš„é¢„è®¾é…ç½®åç§°')

    # æ•°æ®å‚æ•°
    parser.add_argument('--data_path', type=str,
                       default='',  # å°†ä»é…ç½®æ–‡ä»¶è¯»å–ï¼Œå¦‚æœæœªæŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
                       help='æ•°æ®æ–‡ä»¶è·¯å¾„ (é»˜è®¤ä»config.yamlè¯»å–)')
    parser.add_argument('--max_samples', type=int, default=None,
                       help='æœ€å¤§æ ·æœ¬æ•° (Noneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰æ ·æœ¬)')
    parser.add_argument('--num_probes', type=int, default=None,
                       help='æ¢é’ˆæ•°é‡ï¼ˆé»˜è®¤ä»config.yamlè¯»å–ï¼‰')

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=20,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=5e-4,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--batch_size', type=int, default=None,
                       help='æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤ä»é…ç½®æ–‡ä»¶è¯»å–ï¼‰')

    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output_dir', type=str, default=None,
                       help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä»é…ç½®æ–‡ä»¶è¯»å–ï¼‰')
    parser.add_argument('--checkpoint_dir', type=str, default=None,
                       help='æ£€æŸ¥ç‚¹ç›®å½•ï¼ˆé»˜è®¤ä»é…ç½®æ–‡ä»¶è¯»å–ï¼‰')

    # æ˜¾ç¤ºå‚æ•°
    parser.add_argument('--display_every', type=int, default=2,
                       help='æ˜¾ç¤ºé¢‘ç‡')
    parser.add_argument('--save_interval', type=int, default=5,
                       help='ä¿å­˜é—´éš”')
    parser.add_argument('--plot_frequency', type=int, default=2,
                       help='å›¾ç‰‡ä¿å­˜é¢‘ç‡')

    # æ¢å¤è®­ç»ƒ
    parser.add_argument('--resume', type=str, default=None,
                       help='æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„')

    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    print("=== ä¿®å¤ç‰ˆDeepONetç”µç£åœºé‡æ„è®­ç»ƒ ===")
    print("ä¿®å¤å˜é•¿åºåˆ—å’ŒReduceLROnPlateaué—®é¢˜")
    print("æ”¯æŒYAMLé…ç½®æ–‡ä»¶å’Œå‡¸è¾¹ç•Œç­›é€‰")
    print("=" * 60)

    # è§£æå‚æ•°
    args = parse_args()

    # ä»YAMLé…ç½®æ–‡ä»¶åŠ è½½é…ç½®
    config_file = args.config_file if Path(args.config_file).exists() else None
    cfg = Config(config_file=config_file, preset_name=args.preset)

    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®æ–‡ä»¶
    if args.data_path:
        cfg.data.data_path = args.data_path
    elif hasattr(cfg, 'paths') and not cfg.data.data_path:
        # å¦‚æœå‘½ä»¤è¡Œæ²¡æœ‰æŒ‡å®šä¸”dataä¸ºç©ºï¼Œä»pathsè¯»å–
        cfg.data.data_path = cfg.paths.data_path

    if args.max_samples:
        cfg.data.max_samples = args.max_samples
    if hasattr(args, 'num_probes') and args.num_probes:
        cfg.data.num_probes = args.num_probes
        # å…³é”®ä¿®å¤ï¼šåŒæ­¥æ›´æ–°deeponeté…ç½®ä¸­çš„æ¢é’ˆæ•°é‡å’Œè¾“å…¥ç»´åº¦
        cfg.sync_probe_count()
        print(f"[æ’æŸ¥] æ¢é’ˆæ•°é‡é…ç½®è¦†ç›–: data.num_probes={cfg.data.num_probes}, deeponet.probe_count={cfg.deeponet.probe_count}, deeponet.input_dim={cfg.deeponet.input_dim}")

    cfg.training.epochs = args.epochs
    if args.lr:
        cfg.training.learning_rate = args.lr
    if args.batch_size is not None:
        cfg.training.batch_size = args.batch_size
    cfg.training.display_every = args.display_every
    cfg.training.save_interval = args.save_interval

    if hasattr(args, 'plot_frequency') and args.plot_frequency:
        cfg.training.plot_frequency = args.plot_frequency

    if args.output_dir:
        cfg.visualization.output_dir = args.output_dir
    if args.checkpoint_dir:
        cfg.data.checkpoint_dir = args.checkpoint_dir

    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(cfg.visualization.output_dir).mkdir(exist_ok=True)
    Path(cfg.data.checkpoint_dir).mkdir(exist_ok=True)

    # æ‰“å°é…ç½®
    cfg.print_config()

    try:
        # å‡†å¤‡æ•°æ®
        print("å‡†å¤‡ä¿®å¤ç‰ˆæ•°æ®é›†...")
        dataset = MaskedDeepONetDataset(cfg)

        sample_files = dataset.scan_available_samples(
            cfg.data.data_path,
            max_frequency=cfg.data.max_frequency,
            max_samples=cfg.data.max_samples
        )

        if not sample_files:
            raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®æ–‡ä»¶")

        print(f"æ€»å…±å‘ç° {len(sample_files)} ä¸ªå¯ç”¨æ„å‹")

        # é™åˆ¶æ ·æœ¬æ•°é‡
        if cfg.data.max_samples and len(sample_files) > cfg.data.max_samples:
            sample_files = sample_files[:cfg.data.max_samples]
            print(f"é™åˆ¶æ ·æœ¬æ•°é‡ä¸º: {len(sample_files)}")

        # éšæœºæ‰“ä¹±æ ·æœ¬
        np.random.shuffle(sample_files)

        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç´¢å¼•
        print(f"[DEBUG] Total sample files: {len(sample_files)}")

        # å½“æ ·æœ¬æ•°é‡å¾ˆå°‘æ—¶ï¼Œä½¿ç”¨ä¸åŒçš„åˆ’åˆ†ç­–ç•¥
        if len(sample_files) <= 2:
            # æ ·æœ¬å¤ªå°‘æ—¶ï¼Œè‡³å°‘ä¿è¯æ¯ä¸ªé›†åˆæœ‰1ä¸ªæ ·æœ¬
            if len(sample_files) == 1:
                train_samples = sample_files[:1]  # å”¯ä¸€æ ·æœ¬ç»™è®­ç»ƒé›†
                test_samples = sample_files[:1]   # æµ‹è¯•é›†ä¹Ÿç”¨åŒä¸€ä¸ªæ ·æœ¬
                print("[INFO] æ ·æœ¬æ•°é‡å¾ˆå°‘ï¼Œè®­ç»ƒé›†å’Œæµ‹è¯•é›†ä½¿ç”¨åŒä¸€ä¸ªæ ·æœ¬")
            elif len(sample_files) == 2:
                train_samples = sample_files[:1]  # ç¬¬ä¸€ä¸ªæ ·æœ¬ç»™è®­ç»ƒé›†
                test_samples = sample_files[1:]   # ç¬¬äºŒä¸ªæ ·æœ¬ç»™æµ‹è¯•é›†
                print("[INFO] ä½¿ç”¨1:1åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†")
        else:
            # æ­£å¸¸çš„80%/20%åˆ’åˆ†
            split_idx = int(len(sample_files) * 0.8)
            if split_idx == 0:
                split_idx = 1  # ç¡®ä¿è®­ç»ƒé›†è‡³å°‘æœ‰1ä¸ªæ ·æœ¬
            if split_idx == len(sample_files):
                split_idx = len(sample_files) - 1  # ç¡®ä¿æµ‹è¯•é›†è‡³å°‘æœ‰1ä¸ªæ ·æœ¬

            train_samples = sample_files[:split_idx]
            test_samples = sample_files[split_idx:]
            print(f"[INFO] ä½¿ç”¨æ ‡å‡†80%/20%åˆ’åˆ†ï¼Œsplit_idx: {split_idx}")

        print(f"è®­ç»ƒé›†æ„å‹æ•°: {len(train_samples)}")
        print(f"æµ‹è¯•é›†æ„å‹æ•°: {len(test_samples)}")

        # åˆ›å»ºå¯åŠ¨æ€åŠ è½½çš„æ•°æ®é›†
        train_dataset = DynamicDeepONetDataset(train_samples, cfg)
        test_dataset = DynamicDeepONetDataset(test_samples, cfg)

        # è°ƒè¯•é€‰é¡¹ï¼šå›ºå®šæ¢é’ˆé€‰æ‹©ä»¥æµ‹è¯•æ¢é’ˆlossæ”¶æ•›
        # è®¾ç½®ä¸ºTrueå¯ä»¥æ¶ˆé™¤æ¢é’ˆé€‰æ‹©å™ªå£°ï¼Œå¸®åŠ©éªŒè¯æ¢é’ˆlossæ˜¯å¦èƒ½å¿«é€Ÿä¸‹é™
        train_dataset._fixed_probe_mode = False  # å¯ä»¥æ”¹ä¸ºTrueè¿›è¡Œè°ƒè¯•
        test_dataset._fixed_probe_mode = False   # æµ‹è¯•é›†ä¿æŒéšæœº

        print(f"[æ’æŸ¥] æ•°æ®é›†æ¢é’ˆé…ç½®éªŒè¯:")
        print(f"  train_dataset.dataset.probe_count={train_dataset.dataset.probe_count}")
        print(f"  cfg.data.num_probes={cfg.data.num_probes}")
        print(f"  cfg.deeponet.probe_count={cfg.deeponet.probe_count}")
        print(f"  é¢„æœŸbranchè¾“å…¥ç»´åº¦={cfg.data.num_probes * 5}")
        print(f"  å›ºå®šæ¢é’ˆæ¨¡å¼={train_dataset._fixed_probe_mode}")

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆä½¿ç”¨è‡ªå®šä¹‰collateå‡½æ•°ï¼‰
        from torch.utils.data import SubsetRandomSampler, SequentialSampler
        import random

        # æ–¹æ¡ˆ1ï¼šä½¿ç”¨SubsetRandomSamplerè¿›è¡ŒçœŸæ­£çš„éšæœºé‡‡æ ·
        train_indices = list(range(len(train_dataset)))
        train_sampler = SubsetRandomSampler(train_indices)  # âœ… çœŸæ­£çš„éšæœºé‡‡æ ·

        # ä½¿ç”¨è¾ƒå°‘çš„workersä»¥é¿å…CUDA multiprocessé—®é¢˜
        num_workers = 0  # æš‚æ—¶ç¦ç”¨å¤šè¿›ç¨‹é¿å…CUDAå†²çªï¼Œç¡®ä¿è®­ç»ƒç¨³å®šè¿è¡Œ

        print(f"ä½¿ç”¨ {num_workers} ä¸ªæ•°æ®åŠ è½½è¿›ç¨‹")
        print(f"è®­ç»ƒé›†ä½¿ç”¨SubsetRandomSamplerè¿›è¡ŒçœŸæ­£çš„éšæœºé‡‡æ ·")

        # åˆ›å»ºlambdaå‡½æ•°æ¥ä¼ é€’é…ç½®
        train_collate_fn = lambda batch: collate_fn(batch, cfg)
        test_collate_fn = lambda batch: collate_fn(batch, cfg)

        train_loader = DataLoader(
            train_dataset,
            batch_size=cfg.training.batch_size,
            sampler=train_sampler,  # ä½¿ç”¨SubsetRandomSampler
            num_workers=num_workers,  # å¯ç”¨å¤šè¿›ç¨‹æ•°æ®åŠ è½½
            pin_memory=False,  # ç¦ç”¨pin_memoryé¿å…CUDAå†²çª
            collate_fn=train_collate_fn  # å…³é”®ï¼šä½¿ç”¨è‡ªå®šä¹‰collateå‡½æ•°
        )

        test_loader = DataLoader(
            test_dataset,
            batch_size=cfg.training.batch_size,
            sampler=SequentialSampler(range(len(test_dataset))),  # æµ‹è¯•é›†ä¸éœ€è¦shuffle
            num_workers=num_workers,  # å¯ç”¨å¤šè¿›ç¨‹æ•°æ®åŠ è½½
            pin_memory=False,  # ç¦ç”¨pin_memoryé¿å…CUDAå†²çª
            collate_fn=test_collate_fn  # å…³é”®ï¼šä½¿ç”¨è‡ªå®šä¹‰collateå‡½æ•°
        )

        print(f"è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
        print(f"æµ‹è¯•æ‰¹æ¬¡æ•°: {len(test_loader)}")

        # éªŒè¯æ•°æ®åŠ è½½å™¨ä¸ä¸ºç©º
        if len(train_loader) == 0:
            raise ValueError("è®­ç»ƒæ•°æ®åŠ è½½å™¨ä¸ºç©ºï¼")
        if len(test_loader) == 0:
            raise ValueError("æµ‹è¯•æ•°æ®åŠ è½½å™¨ä¸ºç©ºï¼")

        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = ImprovedDeepONetTrainer(cfg)

        # æ¢å¤è®­ç»ƒï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if args.resume:
            try:
                epoch, loss_history, config_dict = load_checkpoint(
                    args.resume, trainer.model, trainer.optimizer, trainer.device
                )
                print(f"ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ: Epoch {epoch}")
            except Exception as e:
                print(f"æ¢å¤è®­ç»ƒå¤±è´¥: {e}")
                return

        # å¼€å§‹ä¼˜åŒ–ç‰ˆè®­ç»ƒ
        train_losses, test_losses = trainer.train_optimized(train_loader, test_loader)

        print("\nä¼˜åŒ–ç‰ˆè®­ç»ƒæˆåŠŸå®Œæˆï¼")
        print(f"ç»“æœä¿å­˜åœ¨: {cfg.visualization.output_dir}")
        print(f"æ£€æŸ¥ç‚¹ä¿å­˜åœ¨: {cfg.data.checkpoint_dir}")

        # è¾“å‡ºæ€§èƒ½æå‡æ€»ç»“
        print("\n=== æ€§èƒ½ä¼˜åŒ–æ€»ç»“ ===")
        print("1. ä¿®å¤æ¨¡å‹é€ç‚¹å¤„ç†bug: O(N) â†’ O(1) å¸¸æ•°æ—¶é—´")
        print("2. ç§»é™¤è®­ç»ƒå¾ªç¯ä¸­çš„æ ·æœ¬çº§forå¾ªç¯: çœŸæ­£çš„æ‰¹é‡å¤„ç†")
        print("3. å……åˆ†åˆ©ç”¨GPUå¹¶è¡Œè®¡ç®—èƒ½åŠ›")
        print("4. é¢„æœŸæ€§èƒ½æå‡: 10-100å€åŠ é€Ÿ")
        print("==================")

    except Exception as e:
        print(f"ä¿®å¤ç‰ˆè®­ç»ƒå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()